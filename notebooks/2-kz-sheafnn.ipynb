{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import git\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.models.disc_models import DiscreteDiagSheafDiffusion, DiscreteBundleSheafDiffusion, DiscreteGeneralSheafDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x)[data.train_mask]\n",
    "    nll = F.nll_loss(out, data.y[data.train_mask])\n",
    "    loss = nll\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    del out\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, accs, losses, preds = model(data.x), [], [], []\n",
    "        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "            pred = logits[mask].max(1)[1]\n",
    "            acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "\n",
    "            loss = F.nll_loss(logits[mask], data.y[mask])\n",
    "\n",
    "            preds.append(pred.detach().cpu())\n",
    "            accs.append(acc)\n",
    "            losses.append(loss.detach().cpu())\n",
    "        return accs, preds, losses\n",
    "\n",
    "def run_exp(args, dataset, model_cls, fold):\n",
    "    data = dataset[0]\n",
    "    data = get_fixed_splits(data, args['dataset'], fold)\n",
    "    data = data.to(args['device'])\n",
    "\n",
    "    model = model_cls(data.edge_index, args)\n",
    "    model = model.to(args['device'])\n",
    "\n",
    "    sheaf_learner_params, other_params = model.grouped_parameters()\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': sheaf_learner_params, 'weight_decay': args['sheaf_decay']},\n",
    "        {'params': other_params, 'weight_decay': args['weight_decay']}\n",
    "    ], lr=args['lr'])\n",
    "\n",
    "    epoch = 0\n",
    "    best_val_acc = test_acc = 0\n",
    "    best_val_loss = float('inf')\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "    best_epoch = 0\n",
    "    bad_counter = 0\n",
    "\n",
    "    for epoch in range(args['epochs']):\n",
    "        train(model, optimizer, data)\n",
    "\n",
    "        [train_acc, val_acc, tmp_test_acc], preds, [\n",
    "            train_loss, val_loss, tmp_test_loss] = test(model, data)\n",
    "        if fold == 0:\n",
    "            res_dict = {\n",
    "                f'fold{fold}_train_acc': train_acc,\n",
    "                f'fold{fold}_train_loss': train_loss,\n",
    "                f'fold{fold}_val_acc': val_acc,\n",
    "                f'fold{fold}_val_loss': val_loss,\n",
    "                f'fold{fold}_tmp_test_acc': tmp_test_acc,\n",
    "                f'fold{fold}_tmp_test_loss': tmp_test_loss,\n",
    "            }\n",
    "            wandb.log(res_dict, step=epoch)\n",
    "\n",
    "        new_best_trigger = val_acc > best_val_acc if args['stop_strategy'] == 'acc' else val_loss < best_val_loss\n",
    "        if new_best_trigger:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "            test_acc = tmp_test_acc\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "\n",
    "        if bad_counter == args['early_stopping']:\n",
    "            break\n",
    "\n",
    "    print(f\"Fold {fold} | Epochs: {epoch} | Best epoch: {best_epoch}\")\n",
    "    print(f\"Test acc: {test_acc:.4f}\")\n",
    "    print(f\"Best val acc: {best_val_acc:.4f}\")\n",
    "\n",
    "    if \"ODE\" not in args['model']:\n",
    "        # Debugging for discrete models\n",
    "        for i in range(len(model.sheaf_learners)):\n",
    "            L_max = model.sheaf_learners[i].L.detach().max().item()\n",
    "            L_min = model.sheaf_learners[i].L.detach().min().item()\n",
    "            L_avg = model.sheaf_learners[i].L.detach().mean().item()\n",
    "            L_abs_avg = model.sheaf_learners[i].L.detach().abs().mean().item()\n",
    "            print(f\"Laplacian {i}: Max: {L_max:.4f}, Min: {L_min:.4f}, Avg: {L_avg:.4f}, Abs avg: {L_abs_avg:.4f}\")\n",
    "\n",
    "        with np.printoptions(precision=3, suppress=True):\n",
    "            for i in range(0, args['layers']):\n",
    "                print(f\"Epsilons {i}: {model.epsilons[i].detach().cpu().numpy().flatten()}\")\n",
    "\n",
    "    wandb.log({'best_test_acc': test_acc, 'best_val_acc': best_val_acc, 'best_epoch': best_epoch})\n",
    "    keep_running = False if test_acc < args['min_acc'] else True\n",
    "\n",
    "    return test_acc, best_val_acc, keep_running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheafnn",
   "language": "python",
   "name": "sheafnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
