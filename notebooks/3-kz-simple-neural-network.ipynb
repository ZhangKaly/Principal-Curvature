{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 09:38:03.240189: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-14 09:38:05.043034: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-14 09:38:05.043171: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-14 09:38:07.793943: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-14 09:38:07.794481: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-14 09:38:07.794493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.w = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.b = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.w) + self.b\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.w.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.w -= learning_rate * weights_error\n",
    "        self.b -= learning_rate * output_error\n",
    "        return input_error\n",
    "    \n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100   error=0.348945\n",
      "epoch 2/100   error=0.308261\n",
      "epoch 3/100   error=0.297969\n",
      "epoch 4/100   error=0.293994\n",
      "epoch 5/100   error=0.291951\n",
      "epoch 6/100   error=0.290664\n",
      "epoch 7/100   error=0.289734\n",
      "epoch 8/100   error=0.288998\n",
      "epoch 9/100   error=0.288380\n",
      "epoch 10/100   error=0.287841\n",
      "epoch 11/100   error=0.287361\n",
      "epoch 12/100   error=0.286925\n",
      "epoch 13/100   error=0.286526\n",
      "epoch 14/100   error=0.286157\n",
      "epoch 15/100   error=0.285815\n",
      "epoch 16/100   error=0.285497\n",
      "epoch 17/100   error=0.285200\n",
      "epoch 18/100   error=0.284922\n",
      "epoch 19/100   error=0.284661\n",
      "epoch 20/100   error=0.284417\n",
      "epoch 21/100   error=0.284187\n",
      "epoch 22/100   error=0.283972\n",
      "epoch 23/100   error=0.283769\n",
      "epoch 24/100   error=0.283579\n",
      "epoch 25/100   error=0.283399\n",
      "epoch 26/100   error=0.283230\n",
      "epoch 27/100   error=0.283071\n",
      "epoch 28/100   error=0.282922\n",
      "epoch 29/100   error=0.282781\n",
      "epoch 30/100   error=0.282649\n",
      "epoch 31/100   error=0.282524\n",
      "epoch 32/100   error=0.282406\n",
      "epoch 33/100   error=0.282296\n",
      "epoch 34/100   error=0.282192\n",
      "epoch 35/100   error=0.282094\n",
      "epoch 36/100   error=0.282002\n",
      "epoch 37/100   error=0.281915\n",
      "epoch 38/100   error=0.281834\n",
      "epoch 39/100   error=0.281757\n",
      "epoch 40/100   error=0.281685\n",
      "epoch 41/100   error=0.281617\n",
      "epoch 42/100   error=0.281553\n",
      "epoch 43/100   error=0.281492\n",
      "epoch 44/100   error=0.281436\n",
      "epoch 45/100   error=0.281382\n",
      "epoch 46/100   error=0.281332\n",
      "epoch 47/100   error=0.281285\n",
      "epoch 48/100   error=0.281240\n",
      "epoch 49/100   error=0.281198\n",
      "epoch 50/100   error=0.281159\n",
      "epoch 51/100   error=0.281121\n",
      "epoch 52/100   error=0.281086\n",
      "epoch 53/100   error=0.281053\n",
      "epoch 54/100   error=0.281022\n",
      "epoch 55/100   error=0.280992\n",
      "epoch 56/100   error=0.280964\n",
      "epoch 57/100   error=0.280938\n",
      "epoch 58/100   error=0.280913\n",
      "epoch 59/100   error=0.280890\n",
      "epoch 60/100   error=0.280867\n",
      "epoch 61/100   error=0.280846\n",
      "epoch 62/100   error=0.280826\n",
      "epoch 63/100   error=0.280807\n",
      "epoch 64/100   error=0.280789\n",
      "epoch 65/100   error=0.280772\n",
      "epoch 66/100   error=0.280756\n",
      "epoch 67/100   error=0.280741\n",
      "epoch 68/100   error=0.280726\n",
      "epoch 69/100   error=0.280712\n",
      "epoch 70/100   error=0.280699\n",
      "epoch 71/100   error=0.280686\n",
      "epoch 72/100   error=0.280674\n",
      "epoch 73/100   error=0.280662\n",
      "epoch 74/100   error=0.280651\n",
      "epoch 75/100   error=0.280640\n",
      "epoch 76/100   error=0.280630\n",
      "epoch 77/100   error=0.280621\n",
      "epoch 78/100   error=0.280611\n",
      "epoch 79/100   error=0.280602\n",
      "epoch 80/100   error=0.280594\n",
      "epoch 81/100   error=0.280585\n",
      "epoch 82/100   error=0.280577\n",
      "epoch 83/100   error=0.280569\n",
      "epoch 84/100   error=0.280562\n",
      "epoch 85/100   error=0.280555\n",
      "epoch 86/100   error=0.280548\n",
      "epoch 87/100   error=0.280541\n",
      "epoch 88/100   error=0.280534\n",
      "epoch 89/100   error=0.280528\n",
      "epoch 90/100   error=0.280521\n",
      "epoch 91/100   error=0.280515\n",
      "epoch 92/100   error=0.280509\n",
      "epoch 93/100   error=0.280503\n",
      "epoch 94/100   error=0.280498\n",
      "epoch 95/100   error=0.280492\n",
      "epoch 96/100   error=0.280486\n",
      "epoch 97/100   error=0.280481\n",
      "epoch 98/100   error=0.280476\n",
      "epoch 99/100   error=0.280470\n",
      "epoch 100/100   error=0.280465\n",
      "[array([[0.52219619]]), array([[0.53281531]]), array([[0.51058584]]), array([[0.52131013]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from network import *\n",
    "\n",
    "# training data\n",
    "#x_train = np.array([[[0, 0]], [[1, 2]], [[2, 3]], [[3, 4]]])\n",
    "#y_train = np.array([[[1]], [[3]], [[5]], [[7]]])\n",
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "# network\n",
    "net = Network()\n",
    "net.add(FCLayer(2, 3))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(3, 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=100, learning_rate=0.1)\n",
    "\n",
    "# test\n",
    "out = net.predict(x_train)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otenv",
   "language": "python",
   "name": "otenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
